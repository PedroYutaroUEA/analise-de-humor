{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bcd919-384a-4502-96fb-5fc628d30ca8",
   "metadata": {},
   "source": [
    "# Análise de sentimenos usando NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61af40-a668-45fe-9d8b-2ad254f67049",
   "metadata": {},
   "source": [
    "- O seguinte projeto tem como objetivo criar um modelo NLP classificador para analisar o humor dos usuários baseados em seus dados (tweets);\n",
    "- A plataforma escolhida para essa atividade foi o X (Twitter), por meio do dataset público sentiment140 disponível no Kaggle;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd1d56-45f9-4436-be35-287f8077d072",
   "metadata": {},
   "source": [
    "# Equipe:\n",
    "- Pedro Yutaro Mont Morency Nakamura;\n",
    "- Lucas Carvalho dos Santos;\n",
    "- Caio Jorge Da Cunha Queiroz;\n",
    "- Lucas Maciel ;omes;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9cd97-9255-4008-9210-8378c322bcc7",
   "metadata": {},
   "source": [
    "# 1 - Importação de bibliotecas\n",
    "Inicialmente iremos importar as seguintes bibliotecas:\n",
    "- *Pandas + Numnpy*: Manipulação de dados;\n",
    "- *Scikit-learn*: Pré-processamento e criação de métricas do modelo;\n",
    "- *NLTK*: Biblioteca para pre-processamento de texto humano (toolkit de linguagem natural);\n",
    "- *matplotlib*: Criação de gráficos para a vizualização dos resultados;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ebb37e4-7d98-46f5-97e4-521744485a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analise e tratamento de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "# Processamento de linguagem natural\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Bibliotecas para o classificador\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82d6db-855f-4e86-b83d-0b3d1c653352",
   "metadata": {},
   "source": [
    "O Conteudo do dataset contem os seguintes campos:\n",
    "\n",
    "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "ids: The id of the tweet ( 2087)\n",
    "\n",
    "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "user: the user that tweeted (robotickilldozr)\n",
    "\n",
    "text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80fa666-88e5-47b6-8866-bb0dcc6f8984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                                               text  \\\n",
       "0       0  1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1       0  1467810672  is upset that he can't update his Facebook by ...   \n",
       "2       0  1467810917  @Kenichan I dived many times for the ball. Man...   \n",
       "3       0  1467811184    my whole body feels itchy and like its on fire    \n",
       "4       0  1467811193  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "  sentiment  \n",
       "0  negativo  \n",
       "1  negativo  \n",
       "2  negativo  \n",
       "3  negativo  \n",
       "4  negativo  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando o dataset do keagle\n",
    "df = pd.read_csv(\"tweets_data.csv\", encoding=\"latin-1\", header=None)\n",
    "# Renomeando colunas conforme a documentação\n",
    "df.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "# Iremos usar apenas essas colunas em nosso treino\n",
    "df = df[[\"target\", \"id\", \"text\"]]\n",
    "# Conversão de status\n",
    "df[\"sentiment\"] = df[\"target\"].apply(lambda x: \"positivo\" if x >= 3 else \"negativo\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a40c0b-ffe9-47a0-b3d4-ef152e5d3581",
   "metadata": {},
   "source": [
    "Como é possível observar, agora o nosso dataframe possui apenas a polaridade, o texto e o sentimento correspondente\n",
    "\n",
    "Porém é possível que parte desse texto contenha \"ruídos\" como URLs, menções, emojis ou símbolos especiais. Para isso, iremos fazer uma breve limpeza usando nltk para lidar com stopwords:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f955efb-39f7-456f-9bbf-1a4c8ebe8ee2",
   "metadata": {},
   "source": [
    "Também precisamos verificar a possível existencia de tweets duplicados pelo seu ID como se segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef7d60d-bf95-4558-a671-d6d9aa26ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentual de tweets duplicados: 0.105%\n"
     ]
    }
   ],
   "source": [
    "def show_amount_dup(data):\n",
    "    dup = round(sum(data.duplicated(\"id\")/len(data)*100), 3)\n",
    "    print(f\"Percentual de tweets duplicados: {dup}%\")\n",
    "\n",
    "show_amount_dup(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d53132-0406-41df-a0c6-6c27ab6ab84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentual de tweets duplicados: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Removendo tweets duplicados\n",
    "df.drop_duplicates(\"id\", inplace=True)\n",
    "show_amount_dup(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd30b53-cb74-4408-8ad6-49d1df356393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizando dataset pra usar apenas colunas relevantes\n",
    "df = df.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1bf013-5b6d-4f1c-92de-a466883ff9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7fafb2e-f73c-4ef1-8d76-e771beb3e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "reURL = r\"http\\S+|www\\S+|https\\S+\"\n",
    "reMention = r\"@\\w+\"\n",
    "reSpecChars_and_Nums = r\"[^A-Za-z\\s]\"\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    # remove urls\n",
    "    txt = re.sub(reURL, \"\", txt)\n",
    "    txt = re.sub(reMention, \"\", txt)\n",
    "    txt = re.sub(reSpecChars_and_Nums, \"\", txt))\n",
    "    # tokenizar e remover stop words\n",
    "    tokens = word_tokenize(txt.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0677579-db68-41ba-9173-15a7fbc58c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0afd4a8-776a-4712-b30c-d1b6fb4afa6e",
   "metadata": {},
   "source": [
    "# 4. Divisão dos dados\n",
    "Com os dados ja tratados, iremos dividi-los em dados de treino e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79bb71-683d-4968-be7d-30bcbb22e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
